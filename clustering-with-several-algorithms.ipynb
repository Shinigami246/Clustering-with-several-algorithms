{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:34.66177Z","iopub.execute_input":"2022-06-25T16:22:34.662217Z","iopub.status.idle":"2022-06-25T16:22:34.673666Z","shell.execute_reply.started":"2022-06-25T16:22:34.662184Z","shell.execute_reply":"2022-06-25T16:22:34.672889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. <a href=\"#0\">Read the dataset</a>\n2. <a href=\"#2\">Data investigation</a>\n3. <a href=\"#3\">Data preprocessing </a>\n4. <a href=\"#4\">Features transformation </a>\n5. <a href=\"#5\">PCA Vs kernal PCA</a>\n6. <a href=\"#6\">K means</a>\n7. <a href=\"#7\">Hierarchical Clustering</a>\n8. <a href=\"#8\">dbscan</a>\n9. <a href=\"#9\">IsolationForest</a>\n10. <a href=\"#10\">GMM</a>\n11. <a href=\"#11\">Comparison</a>\n12. <a href=\"#12\">chosen algo with t-sne</a>\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.metrics import davies_bouldin_score\nfrom sklearn.manifold import TSNE\nimport plotly.express as px\nimport seaborn as sns\n\n\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:34.675063Z","iopub.execute_input":"2022-06-25T16:22:34.67551Z","iopub.status.idle":"2022-06-25T16:22:34.684035Z","shell.execute_reply.started":"2022-06-25T16:22:34.67548Z","shell.execute_reply":"2022-06-25T16:22:34.68317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. <a name=\"1\">Read the dataset</a>\n(<a href=\"#0\">Go to top</a>)\n\n","metadata":{}},{"cell_type":"code","source":"#read the data\n\ndf = pd.read_csv(\"/kaggle/input/ccdata/CC GENERAL.csv\")\n\nprint('The shape of the dataset is:', df.shape)\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:34.75247Z","iopub.execute_input":"2022-06-25T16:22:34.753046Z","iopub.status.idle":"2022-06-25T16:22:34.807959Z","shell.execute_reply.started":"2022-06-25T16:22:34.753011Z","shell.execute_reply":"2022-06-25T16:22:34.806956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. <a name=\"2\">Data investigation</a>\n(<a href=\"#0\">Go to top</a>)\n\nin this part you need to check the data quality and assess any issues in the data as:\n- null values in each column \n- each column has the proper data type\n- outliers\n- duplicate rows\n- distribution for each column (skewness)\n<br>\n\n**comment each issue you find** ","metadata":{}},{"cell_type":"code","source":"# Let's see the data types and non-null values for each column\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:34.848441Z","iopub.execute_input":"2022-06-25T16:22:34.848862Z","iopub.status.idle":"2022-06-25T16:22:34.865879Z","shell.execute_reply.started":"2022-06-25T16:22:34.848829Z","shell.execute_reply":"2022-06-25T16:22:34.864881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round(df.isnull().sum(axis=0)*100/df.shape[0],2)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:34.93537Z","iopub.execute_input":"2022-06-25T16:22:34.93608Z","iopub.status.idle":"2022-06-25T16:22:34.951016Z","shell.execute_reply.started":"2022-06-25T16:22:34.936046Z","shell.execute_reply":"2022-06-25T16:22:34.950282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This will print basic statistics for numerical columns\ndf.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:35.035303Z","iopub.execute_input":"2022-06-25T16:22:35.036403Z","iopub.status.idle":"2022-06-25T16:22:35.101442Z","shell.execute_reply.started":"2022-06-25T16:22:35.036351Z","shell.execute_reply":"2022-06-25T16:22:35.100537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:35.148708Z","iopub.execute_input":"2022-06-25T16:22:35.149115Z","iopub.status.idle":"2022-06-25T16:22:35.166923Z","shell.execute_reply.started":"2022-06-25T16:22:35.149079Z","shell.execute_reply":"2022-06-25T16:22:35.165799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnumerical_features=[feature for feature in df.columns if df[feature].dtypes!='object']\n\ndf[numerical_features].hist(bins=15, figsize=(20, 20), layout=(6, 3));\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-25T16:22:35.297942Z","iopub.execute_input":"2022-06-25T16:22:35.298365Z","iopub.status.idle":"2022-06-25T16:22:37.816716Z","shell.execute_reply.started":"2022-06-25T16:22:35.298333Z","shell.execute_reply":"2022-06-25T16:22:37.815663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(20,20))\n\nmask = np.triu(np.ones_like(df.corr()))\nheatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='Greens')\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':21}, pad=16);","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:37.818642Z","iopub.execute_input":"2022-06-25T16:22:37.81922Z","iopub.status.idle":"2022-06-25T16:22:39.369991Z","shell.execute_reply.started":"2022-06-25T16:22:37.819171Z","shell.execute_reply":"2022-06-25T16:22:39.368972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. <a name=\"3\">Data preprocessing</a>\n(<a href=\"#0\">Go to top</a>)\n","metadata":{}},{"cell_type":"markdown","source":"### Define below all the issues that you had found in the previous part\n1-CUST_ID  is categorical datatype (suitable to be the index)<br>\n2- null values          <br>\n3- Normalize numerical values          <br>","metadata":{}},{"cell_type":"code","source":"#make a copy for the original dataset\ndf_copy=df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:39.371589Z","iopub.execute_input":"2022-06-25T16:22:39.372065Z","iopub.status.idle":"2022-06-25T16:22:39.377993Z","shell.execute_reply.started":"2022-06-25T16:22:39.372018Z","shell.execute_reply":"2022-06-25T16:22:39.377041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### for each issue adapt this methodology \n- start by defining the solution\n- apply this solution onn the data\n- test the solution to make sure that you have solved the issue","metadata":{}},{"cell_type":"markdown","source":"**First issue**\nCUST_ID is categorical datatype,so it is suitable to be the index ","metadata":{}},{"cell_type":"code","source":"df_copy.set_index('CUST_ID', inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:39.380453Z","iopub.execute_input":"2022-06-25T16:22:39.381088Z","iopub.status.idle":"2022-06-25T16:22:39.392918Z","shell.execute_reply.started":"2022-06-25T16:22:39.381042Z","shell.execute_reply":"2022-06-25T16:22:39.392088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Second issue** :fill the null values with the means","metadata":{}},{"cell_type":"code","source":"#solution \ndf_copy[\"CREDIT_LIMIT\"].fillna(df_copy[\"CREDIT_LIMIT\"].mean(), inplace=True)\ndf_copy[\"MINIMUM_PAYMENTS\"].fillna( df_copy[\"MINIMUM_PAYMENTS\"].mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:39.394433Z","iopub.execute_input":"2022-06-25T16:22:39.395069Z","iopub.status.idle":"2022-06-25T16:22:39.412101Z","shell.execute_reply.started":"2022-06-25T16:22:39.395025Z","shell.execute_reply":"2022-06-25T16:22:39.411165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test \nround(df_copy.isnull().sum(axis=0)*100/df_copy.shape[0],2)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:39.413605Z","iopub.execute_input":"2022-06-25T16:22:39.414168Z","iopub.status.idle":"2022-06-25T16:22:39.433457Z","shell.execute_reply.started":"2022-06-25T16:22:39.414134Z","shell.execute_reply":"2022-06-25T16:22:39.432334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. <a name=\"4\">Features transformation</a>\n(<a href=\"#0\">Go to top</a>)","metadata":{}},{"cell_type":"markdown","source":"\nfor more details on different methods for scaling check these links\n- https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n- https://www.analyticsvidhya.com/blog/2020/07/types-of-feature-transformation-and-scaling/","metadata":{}},{"cell_type":"markdown","source":"Answer here:","metadata":{}},{"cell_type":"code","source":"df_scaled = df_copy.copy()\ncol_names =df_scaled.columns\nfeatures = df_scaled[col_names]\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:39.435143Z","iopub.execute_input":"2022-06-25T16:22:39.436067Z","iopub.status.idle":"2022-06-25T16:22:39.446328Z","shell.execute_reply.started":"2022-06-25T16:22:39.436019Z","shell.execute_reply":"2022-06-25T16:22:39.44553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Standard Scaler\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf_Standard_Scaler = df_copy.copy()\n\ndf_Standard_Scaler[col_names] = scaler.fit_transform(features.values)\ndf_Standard_Scaler.describe().T\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:39.447881Z","iopub.execute_input":"2022-06-25T16:22:39.448766Z","iopub.status.idle":"2022-06-25T16:22:39.531328Z","shell.execute_reply.started":"2022-06-25T16:22:39.44872Z","shell.execute_reply":"2022-06-25T16:22:39.530403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Robust Scaler","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\n\ndf_RobustScaler = df_copy.copy()\ndf_Standard_Scaler\ndf_RobustScaler[col_names] = scaler.fit_transform(features.values)\ndf_RobustScaler.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:39.532724Z","iopub.execute_input":"2022-06-25T16:22:39.53326Z","iopub.status.idle":"2022-06-25T16:22:39.619453Z","shell.execute_reply.started":"2022-06-25T16:22:39.533217Z","shell.execute_reply":"2022-06-25T16:22:39.618284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MinMax Scaler","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ndf_MinMaxScaler = df_copy.copy()\n\nscaler = MinMaxScaler()\ndf_MinMaxScaler[col_names] = scaler.fit_transform(features.values)\ndf_MinMaxScaler.describe()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:39.622451Z","iopub.execute_input":"2022-06-25T16:22:39.622788Z","iopub.status.idle":"2022-06-25T16:22:39.698951Z","shell.execute_reply.started":"2022-06-25T16:22:39.62276Z","shell.execute_reply":"2022-06-25T16:22:39.697972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. <a name=\"5\">PCA Vs kernal PCA</a>\n(<a href=\"#5\">Go to top</a>)","metadata":{}},{"cell_type":"code","source":"pca = PCA()\n#Transform the data\ndf_pca = pca.fit_transform(df_Standard_Scaler)\nplt.plot(df_pca[0], df_pca[1])  # Plot the chart\nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:39.700179Z","iopub.execute_input":"2022-06-25T16:22:39.700512Z","iopub.status.idle":"2022-06-25T16:22:39.931159Z","shell.execute_reply.started":"2022-06-25T16:22:39.700476Z","shell.execute_reply":"2022-06-25T16:22:39.930039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kernel_pca = KernelPCA(n_components=2,kernel='linear')\nkernel_pca_df = kernel_pca.fit_transform(df_Standard_Scaler)\nplt.plot(kernel_pca_df[0], kernel_pca_df[1])  # Plot the chart\nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:39.932422Z","iopub.execute_input":"2022-06-25T16:22:39.932771Z","iopub.status.idle":"2022-06-25T16:22:42.347331Z","shell.execute_reply.started":"2022-06-25T16:22:39.93274Z","shell.execute_reply":"2022-06-25T16:22:42.346373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. <a name=\"6\">K Means</a>\n(<a href=\"#5\">Go to top</a>)","metadata":{}},{"cell_type":"markdown","source":" 3. Use elbow method\n","metadata":{}},{"cell_type":"code","source":"# 3. Use elbow method\ninertia_list=[]\n\nfor i in range(1, 10):\n    kmean_skl = KMeans(n_clusters=i, n_init=1,max_iter=200)\n    kmean_skl.fit(df_Standard_Scaler)\n    inertia_list.append(kmean_skl.inertia_)\n    \nplt.plot(range(1, 10), inertia_list, marker='o')\nplt.xlabel('Num_of Clusters')\nplt.ylabel('Distortion')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:42.348546Z","iopub.execute_input":"2022-06-25T16:22:42.348873Z","iopub.status.idle":"2022-06-25T16:22:44.113148Z","shell.execute_reply.started":"2022-06-25T16:22:42.348843Z","shell.execute_reply":"2022-06-25T16:22:44.112044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kmean(df):\n    kmean = KMeans(n_clusters=4, max_iter=100)\n    kmean.fit(df)\n    kmean.fit_predict(df)\n    labels_kmean= kmean.labels_\n    #df_pred=pd.DataFrame(pred,index=df_copy.index,columns= ['Model_label'])\n    return labels_kmean","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-25T16:22:44.11425Z","iopub.execute_input":"2022-06-25T16:22:44.114549Z","iopub.status.idle":"2022-06-25T16:22:44.119724Z","shell.execute_reply.started":"2022-06-25T16:22:44.114522Z","shell.execute_reply":"2022-06-25T16:22:44.118768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. <a name=\"7\">Hierarchical Clustering</a>\n(<a href=\"#0\">Go to top</a>)\n","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as shc\n\ndef cluster_Hierarchical(df):\n    cluster_Hierarchical = AgglomerativeClustering(n_clusters=4, affinity='euclidean')\n    cluster_Hierarchical.fit_predict(df)     \n    labels_cluster_Hierarchical=cluster_Hierarchical.labels_\n    return labels_cluster_Hierarchical\n\"\"\"\n#linkage='complete'\nplt.figure(figsize=(10, 7))\nplt.title(\"Counters Dendograms\")\ndend = shc.dendrogram(shc.linkage(y=df_pca , method='complete',metric='euclidean')) \"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:44.120809Z","iopub.execute_input":"2022-06-25T16:22:44.121364Z","iopub.status.idle":"2022-06-25T16:22:44.137687Z","shell.execute_reply.started":"2022-06-25T16:22:44.121334Z","shell.execute_reply":"2022-06-25T16:22:44.136719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. <a name=\"8\">dbscan</a>\n(<a href=\"#0\">Go to top</a>)\n","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\ndef dbscan(df):\n    cluster_dbscan = DBSCAN(eps=8, min_samples=4).fit(df)\n    n=cluster_dbscan.labels_\n    return  cluster_dbscan.labels_\n\n\nn=dbscan(df_RobustScaler)\nn.shape\nunique, counts = np.unique(n, return_counts=True)\ndict(zip(unique, counts))\nprint('Silhoutte score of dbscan is ' , silhouette_score(df_RobustScaler,n))","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:44.139317Z","iopub.execute_input":"2022-06-25T16:22:44.140123Z","iopub.status.idle":"2022-06-25T16:22:47.146095Z","shell.execute_reply.started":"2022-06-25T16:22:44.140083Z","shell.execute_reply":"2022-06-25T16:22:47.145027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\ndef dbscan(df):\n    cluster_dbscan = DBSCAN(eps=8, min_samples=4).fit(df)\n    return  cluster_dbscan.labels_\n\nn=dbscan(df_RobustScaler)\nn.shape\nunique, counts = np.unique(n, return_counts=True)\nprint('Silhoutte score of dbscan is ' , silhouette_score(df_RobustScaler,n))\ndict(zip(unique, counts))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:47.147465Z","iopub.execute_input":"2022-06-25T16:22:47.147794Z","iopub.status.idle":"2022-06-25T16:22:49.941116Z","shell.execute_reply.started":"2022-06-25T16:22:47.147765Z","shell.execute_reply":"2022-06-25T16:22:49.939991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. <a name=\"9\">IsolationForest</a>\n(<a href=\"#0\">Go to top</a>)\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest \ndef IForest(df):\n    lforest = IsolationForest().fit(df)\n    lforest_labels = lforest.predict(df)\n    return lforest_labels","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:49.942259Z","iopub.execute_input":"2022-06-25T16:22:49.942582Z","iopub.status.idle":"2022-06-25T16:22:49.947848Z","shell.execute_reply.started":"2022-06-25T16:22:49.942555Z","shell.execute_reply":"2022-06-25T16:22:49.946895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10. <a name=\"10\">EM</a>\n(<a href=\"#0\">Go to top</a>)\n","metadata":{}},{"cell_type":"code","source":"from sklearn import mixture\ndef gmm(df):\n    gmm = mixture.GaussianMixture(n_components=3,covariance_type=\"full\",max_iter = 100,init_params=\"random\")\n    gmm.fit(df)\n    gmm_labels = gmm.predict(df)\n    return(gmm_labels)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:49.949084Z","iopub.execute_input":"2022-06-25T16:22:49.949415Z","iopub.status.idle":"2022-06-25T16:22:49.961925Z","shell.execute_reply.started":"2022-06-25T16:22:49.949386Z","shell.execute_reply":"2022-06-25T16:22:49.960684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11. <a name=\"11\">Comparison </a>\n#### (K_means,EM,Hierarchical Clustering, IsolationForest,dbscan)\nPCA Vs kernal PCA \n\nMinMaxScaler Vs RobustScaler Vs Standard_Scaler\n\nusing Silhoutte score  and davies bouldin score \n\n(<a href=\"#0\">Go to top</a>)","metadata":{}},{"cell_type":"markdown","source":"#### First let's see the score of these algo with pca and different data transformation\n* df_MinMaxScaler\n* df_RobustScaler \n* df_Standard_Scaler","metadata":{}},{"cell_type":"code","source":"#1-MinMaxScaler with diff algo\ndf_pca = pca.fit_transform(df_MinMaxScaler)\nlabels_kmean=kmean(df_pca)\nlabels_cluster_Hierarchical=cluster_Hierarchical(df_pca)\ngmm_labels=gmm(df_pca)\ncluster_dbscan_labels=dbscan(df_pca)\nlforest_labels=IForest(df_pca)\nprint(\"scores of pca with MinMaxScaler\")\nprint('Silhoutte score of kmean is ' , silhouette_score(df_pca, labels_kmean))\nprint('Silhoutte score of Hierarchical is ' , silhouette_score(df_pca, labels_cluster_Hierarchical))\nprint('Silhoutte score of EM is ' , silhouette_score(df_pca,gmm_labels))\nprint('Silhoutte score of IsolationForest is ' , silhouette_score(df_pca,lforest_labels))\n\nprint('davies bouldin score of kmean is ' , davies_bouldin_score(df_pca, labels_kmean))\nprint('davies bouldin score of Hierarchical is ' , davies_bouldin_score(df_pca, labels_cluster_Hierarchical))\nprint('davies bouldin score of EM is ' , davies_bouldin_score(df_pca, gmm_labels))\nprint('davies bouldin score of IsolationForest is ' , davies_bouldin_score(df_pca, lforest_labels))\n\n#print('Silhoutte score of dbscan is ' , silhouette_score(df_pca,cluster_dbscan_labels))\n\n#2-RobustScaler with diff algo\ndf_pca = pca.fit_transform(df_RobustScaler)\nlabels_kmean=kmean(df_pca)\nlabels_cluster_Hierarchical=cluster_Hierarchical(df_pca)\ngmm_labels=gmm(df_pca)\ncluster_dbscan_labels=dbscan(df_pca)\nlforest_labels=IForest(df_pca)\nprint(\"\\nscores of pca with RobustScaler\")\nprint('Silhoutte score of kmean is ' , silhouette_score(df_pca, labels_kmean))\nprint('Silhoutte score of Hierarchical is ' , silhouette_score(df_pca, labels_cluster_Hierarchical))\nprint('Silhoutte score of EM is ' , silhouette_score(df_pca,gmm_labels))\nprint('Silhoutte score of IsolationForest is ' , silhouette_score(df_pca,lforest_labels))\nprint('Silhoutte score of dbscan is ' , silhouette_score(df_pca,cluster_dbscan_labels))\n\n\nprint('davies bouldin score of kmean is ' , davies_bouldin_score(df_pca, labels_kmean))\nprint('davies bouldin score of Hierarchical is ' , davies_bouldin_score(df_pca, labels_cluster_Hierarchical))\nprint('davies bouldin score of EM is ' , davies_bouldin_score(df_pca, gmm_labels))\nprint('davies bouldin score of IsolationForest is ' , davies_bouldin_score(df_pca, lforest_labels))\nprint('davies bouldin score of dbscan is ' , davies_bouldin_score(df_pca,cluster_dbscan_labels))\n\n\n#3-Standard_Scaler with diff algo\ndf_pca = pca.fit_transform(df_Standard_Scaler)\nlabels_kmean=kmean(df_pca)\nlabels_cluster_Hierarchical=cluster_Hierarchical(df_pca)\ngmm_labels=gmm(df_pca)\ncluster_dbscan_labels=dbscan(df_pca)\nlforest_labels=IForest(df_pca)\nprint(\"\\nscores of pca with Standard_Scaler\")\nprint('Silhoutte score of kmean is ' , silhouette_score(df_pca, labels_kmean))\nprint('Silhoutte score of Hierarchical is ' , silhouette_score(df_pca, labels_cluster_Hierarchical))\nprint('Silhoutte score of EM is ' , silhouette_score(df_pca,gmm_labels))\nprint('Silhoutte score of IsolationForest is ' , silhouette_score(df_pca,lforest_labels))\nprint('Silhoutte score of dbscan is ' , silhouette_score(df_pca,cluster_dbscan_labels))\n\nprint('davies bouldin score of kmean is ' , davies_bouldin_score(df_pca, labels_kmean))\nprint('davies bouldin score of Hierarchical is ' , davies_bouldin_score(df_pca, labels_cluster_Hierarchical))\nprint('davies bouldin score of EM is ' , davies_bouldin_score(df_pca, gmm_labels))\nprint('davies bouldin score of IsolationForest is ' , davies_bouldin_score(df_pca, lforest_labels))\nprint('davies bouldin score of dbscan is ' , davies_bouldin_score(df_pca,cluster_dbscan_labels))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:22:49.965308Z","iopub.execute_input":"2022-06-25T16:22:49.965701Z","iopub.status.idle":"2022-06-25T16:23:38.949164Z","shell.execute_reply.started":"2022-06-25T16:22:49.965667Z","shell.execute_reply":"2022-06-25T16:23:38.94801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Second let's see the score of these algo with pca kernal  and different data transformation\n* df_MinMaxScaler\n* df_RobustScaler \n* df_Standard_Scaler","metadata":{}},{"cell_type":"code","source":"#1-MinMaxScaler with diff algo\ndf_pca = kernel_pca.fit_transform(df_MinMaxScaler)\nlabels_kmean=kmean(df_pca)\nlabels_cluster_Hierarchical=cluster_Hierarchical(df_pca)\ngmm_labels=gmm(df_pca)\ncluster_dbscan_labels=dbscan(df_pca)\nlforest_labels=IForest(df_pca)\nprint(\"scores of kernel_pca with MinMaxScaler\")\nprint('Silhoutte score of kmean is ' , silhouette_score(df_pca, labels_kmean))\nprint('Silhoutte score of Hierarchical is ' , silhouette_score(df_pca, labels_cluster_Hierarchical))\nprint('Silhoutte score of EM is ' , silhouette_score(df_pca,gmm_labels))\nprint('Silhoutte score of IsolationForest is ' , silhouette_score(df_pca,lforest_labels))\n\nprint('\\ndavies bouldin score of kmean is ' , davies_bouldin_score(df_pca, labels_kmean))\nprint('davies bouldin score of Hierarchical is ' , davies_bouldin_score(df_pca, labels_cluster_Hierarchical))\nprint('davies bouldin score of EM is ' , davies_bouldin_score(df_pca, gmm_labels))\nprint('davies bouldin score of IsolationForest is ' , davies_bouldin_score(df_pca, lforest_labels))\n#print('Silhoutte score of dbscan is ' , silhouette_score(df_pca,cluster_dbscan_labels))\n\n#2-RobustScaler with diff algo\ndf_pca = kernel_pca.fit_transform(df_RobustScaler)\nlabels_kmean=kmean(df_pca)\nlabels_cluster_Hierarchical=cluster_Hierarchical(df_pca)\ngmm_labels=gmm(df_pca)\ncluster_dbscan_labels=dbscan(df_pca)\nlforest_labels=IForest(df_pca)\nprint(\"\\n\\nscores of kernel_pca with RobustScaler\")\nprint('Silhoutte score of kmean is ' , silhouette_score(df_pca, labels_kmean))\nprint('Silhoutte score of Hierarchical is ' , silhouette_score(df_pca, labels_cluster_Hierarchical))\nprint('Silhoutte score of EM is ' , silhouette_score(df_pca,gmm_labels))\nprint('Silhoutte score of IsolationForest is ' , silhouette_score(df_pca,lforest_labels))\nprint('Silhoutte score of dbscan is ' , silhouette_score(df_pca,cluster_dbscan_labels))\n\nprint('\\ndavies bouldin score of kmean is ' , davies_bouldin_score(df_pca, labels_kmean))\nprint('davies bouldin score of Hierarchical is ' , davies_bouldin_score(df_pca, labels_cluster_Hierarchical))\nprint('davies bouldin score of EM is ' , davies_bouldin_score(df_pca, gmm_labels))\nprint('davies bouldin score of IsolationForest is ' , davies_bouldin_score(df_pca, lforest_labels))\nprint('davies bouldin score of dbscan is ' , davies_bouldin_score(df_pca,cluster_dbscan_labels))\n\n#3-Standard_Scaler with diff algo\ndf_pca = kernel_pca.fit_transform(df_Standard_Scaler)\nlabels_kmean=kmean(df_pca)\nlabels_cluster_Hierarchical=cluster_Hierarchical(df_pca)\ngmm_labels=gmm(df_pca)\ncluster_dbscan_labels=dbscan(df_pca)\nlforest_labels=IForest(df_pca)\nprint(\"\\n\\nscores of kernel_pca with Standard_Scaler\")\nprint('Silhoutte score of kmean is ' , silhouette_score(df_pca, labels_kmean))\nprint('Silhoutte score of Hierarchical is ' , silhouette_score(df_pca, labels_cluster_Hierarchical))\nprint('Silhoutte score of EM is ' , silhouette_score(df_pca,gmm_labels))\nprint('Silhoutte score of IsolationForest is ' , silhouette_score(df_pca,lforest_labels))\nprint('Silhoutte score of dbscan is ' , silhouette_score(df_pca,cluster_dbscan_labels))\n\nprint('\\ndavies bouldin score of kmean is ' , davies_bouldin_score(df_pca, labels_kmean))\nprint('davies bouldin score of Hierarchical is ' , davies_bouldin_score(df_pca, labels_cluster_Hierarchical))\nprint('davies bouldin score of EM is ' , davies_bouldin_score(df_pca, gmm_labels))\nprint('davies bouldin score of IsolationForest is ' , davies_bouldin_score(df_pca, lforest_labels))\nprint('davies bouldin score of dbscan is ' , davies_bouldin_score(df_pca,cluster_dbscan_labels))\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:23:38.95512Z","iopub.execute_input":"2022-06-25T16:23:38.955863Z","iopub.status.idle":"2022-06-25T16:24:24.237267Z","shell.execute_reply.started":"2022-06-25T16:23:38.955808Z","shell.execute_reply":"2022-06-25T16:24:24.235995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> These result shows that the  **kernel pca**  is better as it enables dealing with more complex data patterns, which would not be visible under linear transformations alone.\n\n>These result shows also that the **RobustScaler** is better for this data \n\n>The **Silhouette** score used to study the separation distance between the resulting clusters :**kmean with kernel_pca and RobustScaler shows best score  0.73**.( higher values indicating better clustering)\n\n>the **Davies-Bouldin** score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score and **Hierarchical Clustering with kernel_pca and RobustScaler** shows best score **0.67**  then the K_mens .( lower values indicating better clustering)\n\n","metadata":{}},{"cell_type":"markdown","source":"### For the  anomaly detection algorithm\n>dbscan shoes the pest result as :\n* dbscan with kernel_pca and Standard_Scaler get davies bouldin score of dbscan = 0.08825825697115738\n* dbscan with kernel_pca and RobustScaler get Silhoutte score = 0.929918036046784","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11. <a name=\"11\">chosen algo with t-sne</a>\n(<a href=\"#0\">Go to top</a>)\n>The algorithms with the best score shown in the last section I will visualize it using t-sne\n","metadata":{}},{"cell_type":"markdown","source":"### 1- kmean with   RobustScaler ","metadata":{}},{"cell_type":"code","source":"tsne = TSNE(n_components=2).fit_transform(df_RobustScaler)\nlabels_kmean=kmean(tsne)\ndf_RobustScaler[\"cluster\"]=labels_kmean.astype(str)\n\nfor c in df_RobustScaler:\n    grid= sns.FacetGrid(df_RobustScaler, col='cluster')\n    grid.map(plt.hist, c)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:26:17.256803Z","iopub.execute_input":"2022-06-25T16:26:17.25726Z","iopub.status.idle":"2022-06-25T16:27:25.64001Z","shell.execute_reply.started":"2022-06-25T16:26:17.257229Z","shell.execute_reply":"2022-06-25T16:27:25.63902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cl = []\nfor row in df_RobustScaler['cluster']:\n    if row == \"0\" :cl.append(\"People mostly doesn't pay by Cash in Advance mostly Purchase in installment and Purchase Frequently\")\n    elif row ==\"1\":cl.append(\"People  with high Purchase Frequently and use all types of payments \")\n    elif row ==\"2\":cl.append(\"People mostly pay by Cash in Advance with high palance and less Purchase Frequently\")\n    elif row ==\"3\":cl.append(\"People with less Purchases and mostly doesn't Purchase in installment\")\n\n\n\n\nsns.scatterplot(tsne[:,0], tsne[:,1] , hue = cl,s=10,palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:27:25.641523Z","iopub.execute_input":"2022-06-25T16:27:25.641941Z","iopub.status.idle":"2022-06-25T16:27:26.471344Z","shell.execute_reply.started":"2022-06-25T16:27:25.641909Z","shell.execute_reply":"2022-06-25T16:27:26.470527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2- Hierarchical Clustering with   RobustScaler ","metadata":{}},{"cell_type":"code","source":"#df = df_RobustScaler.to_numpy()\nlabels_cluster_Hierarchical=cluster_Hierarchical(tsne)\nsns.scatterplot(tsne[:,0], tsne[:,1] , c = labels_cluster_Hierarchical,s=10,palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:27.851121Z","iopub.execute_input":"2022-06-25T16:28:27.851515Z","iopub.status.idle":"2022-06-25T16:28:31.21359Z","shell.execute_reply.started":"2022-06-25T16:28:27.851483Z","shell.execute_reply":"2022-06-25T16:28:31.212543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3- dbscan with   RobustScaler","metadata":{}},{"cell_type":"code","source":"cluster_dbscan_labels=dbscan(tsne)\nplt.scatter(tsne[:,0], tsne[:,1] , c = cluster_dbscan_labels,s=100)\n#sns.scatterplot(tsne[:,0], tsne[:,1] , c = cluster_dbscan_labels,s=10,palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:35.766772Z","iopub.execute_input":"2022-06-25T16:28:35.767192Z","iopub.status.idle":"2022-06-25T16:28:36.250938Z","shell.execute_reply.started":"2022-06-25T16:28:35.767159Z","shell.execute_reply":"2022-06-25T16:28:36.250036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3- dbscan with   and Standard_Scaler","metadata":{}},{"cell_type":"code","source":"tsne_ss = TSNE(n_components=2).fit_transform(df_Standard_Scaler)\ncluster_dbscan_labels=dbscan(tsne_ss)\nplt.scatter(tsne_ss[:,0], tsne_ss[:,1] , c = cluster_dbscan_labels,s=10 )","metadata":{"execution":{"iopub.status.busy":"2022-06-25T16:28:39.996815Z","iopub.execute_input":"2022-06-25T16:28:39.997233Z","iopub.status.idle":"2022-06-25T16:29:34.783474Z","shell.execute_reply.started":"2022-06-25T16:28:39.997201Z","shell.execute_reply":"2022-06-25T16:29:34.782379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">#### The last 2 dbscan graph shows that dbscan with RobustScaler doesn't get any anomaly as the RobustScaler  robust to outliers that's why it get high score in Silhouette score, but dbscan with and Standard Scaler shows anomaly","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}